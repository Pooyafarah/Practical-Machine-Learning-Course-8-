---
output:
    html_document: default
---
get---
title: "Practical Machine Learning Course Project"
author: "Pooya .F"
date: "December 12, 2017"
output: html_document
---
The goal of this project is to create a machine-learning algorithm that can correctly identify the quality of barbell bicep curls by using data from belt, forearm, arm, and dumbbell monitors. There are five classifications of this exercise, one method is the correct form of the exercise while the other four are common mistakes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

The first step is to load the data.

```{r}
#Load libraries
library("caret")

#Read the training data and replace empty values by NA
trainingDataSet<- read.csv("pml-training.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
testingDataSet<- read.csv("pml-testing.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
dim(trainingDataSet)
dim(testingDataSet)
```
Our data consists of 19622 values of 160 variables.

## Clean the data
We remove columns with missing value.
```{r}
trainingDataSet <- trainingDataSet[,(colSums(is.na(trainingDataSet)) == 0)]
dim(trainingDataSet)
testingDataSet <- testingDataSet[,(colSums(is.na(testingDataSet)) == 0)]
dim(testingDataSet)
```

We reduced our data to 60 variables.

## Preprocess the data
```{r}
numericalsIdx <- which(lapply(trainingDataSet, class) %in% "numeric")

preprocessModel <-preProcess(trainingDataSet[,numericalsIdx],method=c('knnImpute', 'center', 'scale'))
pre_trainingDataSet <- predict(preprocessModel, trainingDataSet[,numericalsIdx])
pre_trainingDataSet$classe <- trainingDataSet$classe

pre_testingDataSet <-predict(preprocessModel,testingDataSet[,numericalsIdx])
```

## Removing the non zero variables
Removing the variables with values near zero, that means that they have not so much meaning in the predictions
```{r}
nzv <- nearZeroVar(pre_trainingDataSet,saveMetrics=TRUE)
pre_trainingDataSet <- pre_trainingDataSet[,nzv$nzv==FALSE]

nzv <- nearZeroVar(pre_testingDataSet,saveMetrics=TRUE)
pre_testingDataSet <- pre_testingDataSet[,nzv$nzv==FALSE]
```

## Validation set
We want a 75% observation training dataset to train our model. We will then validate it on the last 70%.
```{r}
set.seed(12031987)
idxTrain<- createDataPartition(pre_trainingDataSet$classe, p=3/4, list=FALSE)
training<- pre_trainingDataSet[idxTrain, ]
validation <- pre_trainingDataSet[-idxTrain, ]
dim(training) ; dim(validation)
```

## Train Model
We train a model using random forest with a cross validation of 5 folds to avoid overfitting.

```{r}
library(randomForest)
modFitrf <- train(classe ~., method="rf", data=training, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE, importance=TRUE )
modFitrf
```

## Interpretation
Let's plot the importance of each individual variable

```{r}

varImpPlot(modFitrf$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 0.6, main = "Importance of the Individual Principal Components")
```

This plot shows each of the principal components in order from most important to least important.

## Cross Validation Testing and Out-of-Sample Error Estimate
Let's apply our training model on our testing database, to check its accuracy.

### Accuracy and Estimated out of sample error

```{r}
predValidRF <- predict(modFitrf, validation)
confus <- confusionMatrix(validation$classe, predValidRF)
confus$table
```

We can notice that there are very few variables out of this model.

```{r}
accur <- postResample(validation$classe, predValidRF)
modAccuracy <- accur[[1]]
modAccuracy
out_of_sample_error <- 1 - modAccuracy
out_of_sample_error
```

The estimated accuracy of the model is 99.7% and the estimated out-of-sample error based on our fitted model applied to the cross validation dataset is 0.3%.

## Application of this model on the 20 test cases provided
We have already clean the test data base (teData). We delete the "problem id" column as it is useless for our analysis.

```{r}
pred_final <- predict(modFitrf, pre_testingDataSet)
pred_final
```

Here are our results, we will use them for the submission of this course project in the coursera platform.